{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. gensim实践：\n",
    "\n",
    "1. 读取预处理好的数据\n",
    "2. 训练\n",
    "3. 完事"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 数据集路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "merger_data_path = 'data/merged_train_test_seg_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 载入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merger_data_path data size 102871\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>方向机 重 助力 泵 方向机 都 换 新 都 换 助力 泵 方向机 换 方向机 带 助力 重...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>奔驰 ML500 排气 凸轮轴 调节 错误 有没有 电脑 检测 故障 代码 有发 一下 发动...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010 款 宝马X1 2011 年 出厂 20 排量 通用 6L45 变速箱 原地 换挡 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30V6 发动机 号 位置 照片 最好 右侧 排气管 上方 缸体 上 靠近 变速箱 是不是 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012 款 奔驰 c180 维修保养 动力 值得 拥有 家庭 用车 入手 维修保养 费用 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  方向机 重 助力 泵 方向机 都 换 新 都 换 助力 泵 方向机 换 方向机 带 助力 重...\n",
       "1  奔驰 ML500 排气 凸轮轴 调节 错误 有没有 电脑 检测 故障 代码 有发 一下 发动...\n",
       "2  2010 款 宝马X1 2011 年 出厂 20 排量 通用 6L45 变速箱 原地 换挡 ...\n",
       "3  30V6 发动机 号 位置 照片 最好 右侧 排气管 上方 缸体 上 靠近 变速箱 是不是 ...\n",
       "4  2012 款 奔驰 c180 维修保养 动力 值得 拥有 家庭 用车 入手 维修保养 费用 ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merger_df = pd.read_csv(merger_data_path,header=None)\n",
    "print('merger_data_path data size {}'.format(len(merger_df)))\n",
    "merger_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 模型创建\n",
    "\n",
    "Gensim中 Word2Vec 模型的期望输入是进过分词的句子列表，即是某个二维数组。这里我们暂时使用 Python 内置的数组，不过其在输入数据集较大的情况下会占用大量的 RAM。Gensim 本身只是要求能够迭代的有序句子列表，因此在工程实践中我们可以使用自定义的生成器，只在内存中保存单条语句。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec 参数\n",
    "+ min_count\n",
    "\n",
    "在不同大小的语料集中，我们对于基准词频的需求也是不一样的。譬如在较大的语料集中，我们希望忽略那些只出现过一两次的单词，这里我们就可以通过设置min_count参数进行控制。一般而言，合理的参数值会设置在0~100之间。\n",
    "\n",
    "+ size\n",
    "\n",
    "size参数主要是用来设置神经网络的层数，Word2Vec 中的默认值是设置为100层。更大的层次设置意味着更多的输入数据，不过也能提升整体的准确度，合理的设置范围为 10~数百。\n",
    "\n",
    "+ workers\n",
    "\n",
    "workers参数用于设置并发训练时候的线程数，不过仅当Cython安装的情况下才会起作用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roger/.conda/envs/lecture02/lib/python3.6/site-packages/scipy/__init__.py:115: UserWarning: Numpy 1.13.3 or above is required for this version of scipy (detected version 1.13.1)\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# 引入 word2vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models import word2vec\n",
    "import gensim\n",
    "\n",
    "# 引入日志配置\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/merged_train_test_seg_data.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merger_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-18 12:28:24,372 : INFO : collecting all words and their counts\n",
      "2019-11-18 12:28:24,377 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-11-18 12:28:24,627 : INFO : PROGRESS: at sentence #10000, processed 941592 words, keeping 36787 word types\n",
      "2019-11-18 12:28:24,838 : INFO : PROGRESS: at sentence #20000, processed 1897707 words, keeping 54137 word types\n",
      "2019-11-18 12:28:25,045 : INFO : PROGRESS: at sentence #30000, processed 2842327 words, keeping 66972 word types\n",
      "2019-11-18 12:28:25,250 : INFO : PROGRESS: at sentence #40000, processed 3758961 words, keeping 77905 word types\n",
      "2019-11-18 12:28:25,469 : INFO : PROGRESS: at sentence #50000, processed 4736128 words, keeping 87815 word types\n",
      "2019-11-18 12:28:25,698 : INFO : PROGRESS: at sentence #60000, processed 5774810 words, keeping 97787 word types\n",
      "2019-11-18 12:28:25,935 : INFO : PROGRESS: at sentence #70000, processed 6836809 words, keeping 107409 word types\n",
      "2019-11-18 12:28:26,145 : INFO : PROGRESS: at sentence #80000, processed 7783080 words, keeping 115538 word types\n",
      "2019-11-18 12:28:26,341 : INFO : PROGRESS: at sentence #90000, processed 8644550 words, keeping 123459 word types\n",
      "2019-11-18 12:28:26,536 : INFO : PROGRESS: at sentence #100000, processed 9497584 words, keeping 130506 word types\n",
      "2019-11-18 12:28:26,593 : INFO : collected 132525 word types from a corpus of 9748048 raw words and 102871 sentences\n",
      "2019-11-18 12:28:26,593 : INFO : Loading a fresh vocabulary\n",
      "2019-11-18 12:28:26,677 : INFO : effective_min_count=5 retains 32906 unique words (24% of original 132525, drops 99619)\n",
      "2019-11-18 12:28:26,677 : INFO : effective_min_count=5 leaves 9598327 word corpus (98% of original 9748048, drops 149721)\n",
      "2019-11-18 12:28:26,733 : INFO : deleting the raw counts dictionary of 132525 items\n",
      "2019-11-18 12:28:26,736 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2019-11-18 12:28:26,736 : INFO : downsampling leaves estimated 8611423 word corpus (89.7% of prior 9598327)\n",
      "2019-11-18 12:28:26,786 : INFO : estimated required memory for 32906 words and 200 dimensions: 69102600 bytes\n",
      "2019-11-18 12:28:26,787 : INFO : resetting layer weights\n",
      "2019-11-18 12:28:27,078 : INFO : training model with 8 workers on 32906 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-11-18 12:28:28,087 : INFO : EPOCH 1 - PROGRESS: at 23.43% examples, 2009615 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-18 12:28:29,089 : INFO : EPOCH 1 - PROGRESS: at 47.73% examples, 2040583 words/s, in_qsize 13, out_qsize 2\n",
      "2019-11-18 12:28:30,091 : INFO : EPOCH 1 - PROGRESS: at 69.61% examples, 2050948 words/s, in_qsize 14, out_qsize 0\n",
      "2019-11-18 12:28:31,106 : INFO : EPOCH 1 - PROGRESS: at 95.17% examples, 2045927 words/s, in_qsize 13, out_qsize 2\n",
      "2019-11-18 12:28:31,243 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-18 12:28:31,244 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-18 12:28:31,246 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-18 12:28:31,248 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-18 12:28:31,248 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-18 12:28:31,255 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-18 12:28:31,256 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-18 12:28:31,257 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-18 12:28:31,257 : INFO : EPOCH - 1 : training on 9748048 raw words (8611548 effective words) took 4.2s, 2062220 effective words/s\n",
      "2019-11-18 12:28:32,259 : INFO : EPOCH 2 - PROGRESS: at 23.15% examples, 1994801 words/s, in_qsize 15, out_qsize 1\n",
      "2019-11-18 12:28:33,265 : INFO : EPOCH 2 - PROGRESS: at 47.30% examples, 2025531 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-18 12:28:34,266 : INFO : EPOCH 2 - PROGRESS: at 69.91% examples, 2062002 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-18 12:28:35,272 : INFO : EPOCH 2 - PROGRESS: at 95.43% examples, 2056556 words/s, in_qsize 14, out_qsize 0\n",
      "2019-11-18 12:28:35,417 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-18 12:28:35,419 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-18 12:28:35,420 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-18 12:28:35,420 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-18 12:28:35,420 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-18 12:28:35,423 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-18 12:28:35,425 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-18 12:28:35,426 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-18 12:28:35,426 : INFO : EPOCH - 2 : training on 9748048 raw words (8611332 effective words) took 4.2s, 2066545 effective words/s\n",
      "2019-11-18 12:28:36,431 : INFO : EPOCH 3 - PROGRESS: at 23.32% examples, 2008717 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-18 12:28:37,438 : INFO : EPOCH 3 - PROGRESS: at 47.71% examples, 2038997 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-18 12:28:38,440 : INFO : EPOCH 3 - PROGRESS: at 69.61% examples, 2049949 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-18 12:28:39,447 : INFO : EPOCH 3 - PROGRESS: at 94.53% examples, 2038118 words/s, in_qsize 13, out_qsize 2\n",
      "2019-11-18 12:28:39,598 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-18 12:28:39,606 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-18 12:28:39,611 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-18 12:28:39,612 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-18 12:28:39,613 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-18 12:28:39,614 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-18 12:28:39,617 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-18 12:28:39,618 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-18 12:28:39,619 : INFO : EPOCH - 3 : training on 9748048 raw words (8610203 effective words) took 4.2s, 2054894 effective words/s\n",
      "2019-11-18 12:28:40,625 : INFO : EPOCH 4 - PROGRESS: at 23.25% examples, 1996078 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-18 12:28:41,626 : INFO : EPOCH 4 - PROGRESS: at 47.61% examples, 2038047 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-18 12:28:42,629 : INFO : EPOCH 4 - PROGRESS: at 70.02% examples, 2063593 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-18 12:28:43,632 : INFO : EPOCH 4 - PROGRESS: at 94.66% examples, 2044353 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-18 12:28:43,777 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-18 12:28:43,781 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-18 12:28:43,789 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-18 12:28:43,790 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-18 12:28:43,794 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-18 12:28:43,796 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-18 12:28:43,796 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-18 12:28:43,797 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-18 12:28:43,798 : INFO : EPOCH - 4 : training on 9748048 raw words (8611237 effective words) took 4.2s, 2061508 effective words/s\n",
      "2019-11-18 12:28:44,802 : INFO : EPOCH 5 - PROGRESS: at 23.45% examples, 2017103 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-18 12:28:45,807 : INFO : EPOCH 5 - PROGRESS: at 47.80% examples, 2045325 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-18 12:28:46,820 : INFO : EPOCH 5 - PROGRESS: at 70.13% examples, 2058189 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-18 12:28:47,821 : INFO : EPOCH 5 - PROGRESS: at 95.53% examples, 2054429 words/s, in_qsize 12, out_qsize 2\n",
      "2019-11-18 12:28:47,944 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-18 12:28:47,945 : INFO : worker thread finished; awaiting finish of 6 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-18 12:28:47,948 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-18 12:28:47,949 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-18 12:28:47,957 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-18 12:28:47,958 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-18 12:28:47,958 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-18 12:28:47,959 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-18 12:28:47,959 : INFO : EPOCH - 5 : training on 9748048 raw words (8612407 effective words) took 4.2s, 2070204 effective words/s\n",
      "2019-11-18 12:28:47,960 : INFO : training on a 48740240 raw words (43056727 effective words) took 20.9s, 2062033 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model = word2vec.Word2Vec(LineSentence(merger_data_path), workers=8,min_count=5,size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查找最近的词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-18 12:28:47,963 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('名爵', 0.8551163673400879),\n",
       " ('东南', 0.8445316553115845),\n",
       " ('海马', 0.8362988233566284),\n",
       " ('二代', 0.8353272676467896),\n",
       " ('江淮', 0.8290162086486816),\n",
       " ('东风风行', 0.8268694281578064),\n",
       " ('猎豹', 0.8268465995788574),\n",
       " ('铃木', 0.823360025882721),\n",
       " ('瑞虎', 0.8204773664474487),\n",
       " ('帕杰罗', 0.8157345056533813)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(['奇瑞'],topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path='data/wv/word2vec.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-18 12:28:48,018 : INFO : saving Word2Vec object under data/wv/word2vec.model, separately None\n",
      "2019-11-18 12:28:48,021 : INFO : not storing attribute vectors_norm\n",
      "2019-11-18 12:28:48,023 : INFO : not storing attribute cum_table\n",
      "2019-11-18 12:28:48,382 : INFO : saved data/wv/word2vec.model\n"
     ]
    }
   ],
   "source": [
    "model.save(save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 载入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-18 12:28:48,392 : INFO : loading Word2Vec object from data/wv/word2vec.model\n",
      "2019-11-18 12:28:48,715 : INFO : loading wv recursively from data/wv/word2vec.model.wv.* with mmap=None\n",
      "2019-11-18 12:28:48,715 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-11-18 12:28:48,716 : INFO : loading vocabulary recursively from data/wv/word2vec.model.vocabulary.* with mmap=None\n",
      "2019-11-18 12:28:48,716 : INFO : loading trainables recursively from data/wv/word2vec.model.trainables.* with mmap=None\n",
      "2019-11-18 12:28:48,717 : INFO : setting ignored attribute cum_table to None\n",
      "2019-11-18 12:28:48,717 : INFO : loaded data/wv/word2vec.model\n"
     ]
    }
   ],
   "source": [
    "model = word2vec.Word2Vec.load(save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-18 12:28:48,761 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('名爵', 0.8551163673400879),\n",
       " ('东南', 0.8445316553115845),\n",
       " ('海马', 0.8362988233566284),\n",
       " ('二代', 0.8353272676467896),\n",
       " ('江淮', 0.8290162086486816),\n",
       " ('东风风行', 0.8268694281578064),\n",
       " ('猎豹', 0.8268465995788574),\n",
       " ('铃木', 0.823360025882721),\n",
       " ('瑞虎', 0.8204773664474487),\n",
       " ('帕杰罗', 0.8157345056533813)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(['奇瑞'],topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://radimrehurek.com/gensim/models/word2vec.html "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture02",
   "language": "python",
   "name": "lecture02"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
